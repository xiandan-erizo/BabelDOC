source,target,tgt_lng
NVIDIA A100 40GB,NVIDIA A100 40GB,
Parameters,参数,
Cache,缓存,
Parameter size,参数大小,
Existing systems,现有系统,
vLLM,vLLM,
Batch size (# requests),批处理大小（请求数量）,
0.4k,0.4k,
0.8k,0.8k,
Orca,Orca,
Max,最大,
Pow2,Pow2,
Oracle,Oracle,
Token states,令牌状态,
Reservation,预留,
Internal frag.,内部碎片,
External frag.,外部碎片,
& Others,及其他,
Figure 1,图1,
LLM,大语言模型,
13B parameters,130亿参数,
NVIDIA A100,NVIDIA A100,
GPU,GPU,
KV cache,KV缓存,
serving throughput,服务吞吐量,
cost per request,每次请求的成本,
autoregressive Transformer model,自回归Transformer模型,
tokens,令牌,
prompt,提示,
termination token,终止词元,
memory-bound,内存受限,
computation power,计算能力,
batching,批处理,
memory space,内存空间,
model weights,模型权重,
dynamic states,动态状态,
key and value tensors,键和值张量,
attention mechanism,注意力机制,
context,上下文,
40GB RAM,40GB内存,
1.2k,1.2千,
Efficient Memory Management for Large Language Model Serving with PagedAttention,使用PagedAttention实现大语言模型服务的高效内存管理,
Woosuk Kwon,权宇锡,
Zhuohan Li,李卓翰,
Siyuan Zhuang,庄思源,
Ying Sheng,盛颖,
Lianmin Zheng,郑廉民,
Cody Hao Yu,余浩德,
Joseph E. Gonzalez,约瑟夫·E·冈萨雷斯,
Hao Zhang,张浩,
Ion Stoica,伊昂·斯托伊卡,
UC Berkeley,加州大学伯克利分校,
Stanford University,斯坦福大学,
Independent Researcher,独立研究员,
UC SanDiego,加州大学圣地亚哥分校,
Abstract,摘要,
High throughput serving of large language models (LLMs),大语言模型（LLM）的高吞吐量服务,
key-value cache (KV cache),键值缓存（KV缓存）,
PagedAttention,分页注意力,
attention algorithm,注意力算法,
virtual memory,虚拟内存,
paging techniques,分页技术,
operating systems,操作系统,
LLM serving system,LLM服务系统,
near-zero waste in KV cache memory,KV缓存内存接近零浪费,
flexible sharing of KV cache,KV缓存的灵活共享,
memory usage,内存使用,
throughput,吞吐量,
latency,延迟,
state-of-the-art systems,最先进的系统,
FasterTransformer,FasterTransformer,
sourcecode,源代码,
https://github.com/vllm-project/vllm,https://github.com/vllm-project/vllm,
Introduction,引言,
GPT,GPT,
PaLM,PaLM,
programming assistants,编程助手,
universal chatbots,通用聊天机器人,
cloud companies,云公司,
hardware accelerators,硬件加速器,
GPUs,GPU,
keyword query,关键词查询,
SOSP ’23,SOSP ’23,
"October 23–26, 2023",2023年10月23日至26日,
"Koblenz, Germany",德国科布伦茨,
ACM ISBN 979-8-4007-0229-7/23/10,ACM ISBN 979-8-4007-0229-7/23/10,
https://doi.org/10.1145/3600006.3613165,https://doi.org/10.1145/3600006.3613165,
OLLAserving settings,OLLAserving设置,
FlashAttention,FlashAttention,
block-level memory management,块级内存管理,
online serving,在线服务,
attention keys and values,注意力键和值,
non-contiguous paged memory,非连续分页内存,
high-throughput LLM serving system,高吞吐量大语言模型服务系统,
efficient memory management,高效内存管理,
copy-on-write,写时复制,
decoding algorithms,解码算法,
LLM serving,大语言模型服务,
throughput improvements,吞吐量提升,
Xiaoxuan Liu,刘晓璇,
Zhifeng Chen,陈志锋,
Yanping Huang,黄艳萍,
Lidong Zhou,周立东,
AndreessenHorowitz,AndreessenHorowitz,
Anyscale,Anyscale,
Astronomer,Astronomer,
Google,谷歌,
IBM,IBM,
Intel,英特尔,
Lacework,Lacework,
Microsoft,微软,
Mohamed Bin Zayed University of Artificial Intelligence,穆罕默德·本·扎耶德人工智能大学,
Samsung SDS,三星SDS,
Uber,优步,
VMware,VMware,
percentage of memory,内存百分比,
activations,激活,
ephemeral tensors,瞬态张量,
GPU memory,GPU内存,
batch size,批处理大小,
Fig. 1,图1,
existing LLM serving systems,现有的大语言模型服务系统,
contiguous memory space,连续内存空间,
deep learning frameworks,深度学习框架,
traditional deep learning workloads,传统深度学习工作负载,
dynamically grows and shrinks,动态增长和缩小,
generates new tokens,生成新令牌,
lifetime,生命周期,
maximum length,最大长度,
internal and external memory fragmentation,内部和外部内存碎片,
pre-allocate,预分配,
actual length,实际长度,
Fig. 11,图11,
request's maximum length,请求的最大长度,
2048 tokens,2048个令牌,
Fig. 2,图2,
memory sharing,内存共享,
parallel sampling,并行采样,
beam search,束搜索,
multiple sequences,多个序列,
operating system's (OS),操作系统（OS）,
virtual memory with paging,带分页的虚拟内存,
blocks,块,
fixed number of tokens,固定数量的令牌,
on demand,按需,
granularity of a block,块粒度,
OPT,OPT,
13B,130亿参数,
66B,660亿参数,
175B,1750亿参数,
LLaMA,LLaMA,
GPT-3,GPT-3,
A2 instances,A2实例,
NVIDIA A100 GPUs,NVIDIA A100 GPU,
Google Cloud Platform,谷歌云平台,
Table 1,表1,
ShareGPT,ShareGPT,
Alpaca,Alpaca,
ChatGPT,ChatGPT,
GPT-3.5,GPT-3.5,
Poisson distribution,泊松分布,
Triton,Triton,
batch size B,批处理大小B,
GPU memory capacity,GPU内存容量,
sequence length,序列长度,
normalized latency,归一化延迟,
end-to-end latency,端到端延迟,
output length,输出长度,
request rates,请求速率,
1-hour traces,1小时跟踪,
15-minute traces,15分钟跟踪,
OPT-175B model,OPT-175B模型,
recomputation,重计算,
swapping,交换,
Fig. 19,图19,
CPU,中央处理器,
PCIe,PCIe,
KV blocks,KV块,
block sizes,块大小,
end-to-end performance,端到端性能,
microbenchmark,微基准测试,
overheads,开销,
paging,分页,
DNN training,深度神经网络训练,
tensor shapes,张量形状,
memory allocation,内存分配,
compute-bound,计算密集型,
memory indirection,内存间接寻址,
non-contiguous block memory,非连续块内存,
vLLM’s techniques,vLLM的技术,
application-specific semantics,应用特定语义,
swap-out policy,换出策略,
all-or-nothing,全有或全无,
recomputation method,重计算方法,
evicted blocks,被驱逐的块,
OS,操作系统,
GPU kernels,GPU内核,
memory access operations,内存访问操作,
attention,注意力,
model serving systems,模型服务系统,
Clipper,Clipper,
TensorFlow Serving,TensorFlow Serving,
Nexus,Nexus,
InferLine,InferLine,
Clockwork,Clockwork,
DVABatch,DVABatch,
REEF,REEF,
Shepherd,Shepherd,
AlpaServe,AlpaServe,
caching,缓存,
placement,放置,
scheduling,调度,
multi-entry multi-exit batching,多入口多出口批处理,
preemption,抢占,
model parallelism,模型并行,
statistical multiplexing,统计复用,
general systems,通用系统,
autoregressive property,自回归特性,
token state,令牌状态,
LLM inference,大语言模型推理,
decoding methods,解码方法,
accessing patterns,访问模式,
simultaneous processing,同时处理,
requests,请求,
decoding preferences,解码偏好,
logical blocks,逻辑块,
physical blocks,物理块,
common mapping layer,通用映射层,
execution kernel,执行内核,
batching opportunities,批处理机会,
sampling requirements,采样需求,
system throughput,系统吞吐量,
request traffic,请求流量,
system capacity,系统容量,
first-come-first-serve (FCFS),先到先服务（FCFS）,
scheduling policy,调度策略,
fairness,公平性,
starvation,饥饿,
input prompts,输入提示,
output lengths,输出长度,
eviction policies,驱逐策略,
heuristics,启发式方法,
all-or-nothing eviction policy,全有或全无驱逐策略,
sequence group,序列组,
beam candidates,束候选,
beam search request,束搜索请求,
gang-scheduled,成组调度,
swap space,交换空间,
disk,磁盘,
CPU memory,CPU内存,
CPU block allocator,CPU块分配器,
GPU block allocator,GPU块分配器,
CPU RAM,CPU内存,
GPU RAM,GPU内存,
new tokens,新令牌,
preempted sequences,被抢占的序列,
request completes,请求完成,
swap space on the CPU RAM,CPU内存上的交换空间,
KV cache memory,KV缓存内存,
Transformer,变压器,
LargeLanguageModels,大语言模型,
language modeling,语言建模,
autoregressive decomposition,自回归分解,
self-attention layers,自注意力层,
input hidden state sequence,输入隐藏状态序列,
"query, key, and value vectors",查询、键和值向量,
attention score,注意力分数,
weighted average,加权平均,
embedding layer,嵌入层,
feed-forward layer,前馈层,
layer normalization,层归一化,
residual connection,残差连接,
output logit computation,输出logit计算,
LLM Service,大语言模型服务,
AutoregressiveGeneration,自回归生成,
conditional generation service,条件生成服务,
completion API,补全API,
chatbot,聊天机器人,
input prompt tokens,输入提示标记,
output tokens,输出标记,
prompt phase,提示阶段,
user prompt,用户提示,
matrix-matrix multiplication,矩阵-矩阵乘法,
Beam candidate 2,Beam候选2,
Beam candidate 3,Beam候选3,
Figure 8,图8,
KVcache,KV缓存,
reference count,引用计数,
generation phase,生成阶段,
copy-on-write mechanism,写时复制机制,
OS virtual memory,操作系统虚拟内存,
Fig. 8,图8,
sample A1,样本A1,
sample A2,样本A2,
Block engine,块引擎,
long input prompts,长输入提示,
machine translation,机器翻译,
beam width parameter k,束宽参数k,
decoding,解码,
top-k most probable sequences,前k个最可能的序列,
k · |V | candidates,k·|V|个候选,
vocabulary size,词汇表大小,
Figure 9,图9,
Joseph,约瑟夫,
Greg Brockman,格雷格·布罗克曼,
arXiv preprint arXiv:2107.03374,arXiv预印本arXiv:2107.03374,
Tianqi Chen,陈天奇,
Bing Xu,徐兵,
Chiyuan Zhang,张驰远,
Carlos Guestrin,卡洛斯·格斯特林,
arXiv preprint arXiv:1604.06174,arXiv预印本arXiv:1604.06174,
Wei-Lin Chiang,江威霖,
Zi Lin,林子,
Zhanghao Wu,吴章浩,
Yonghao Zhuang,庄永豪,
Eric P. Xing,邢波,
Vicuna,Vicuna,
GPT-4,GPT-4,
Aakanksha Chowdhery,阿坎克莎·乔杜里,
Sharan Narang,沙兰·纳朗,
Jacob Devlin,雅各布·德夫林,
Maarten Bosma,马尔滕·博斯马,
Gaurav Mishra,高拉夫·米什拉,
Adam Roberts,亚当·罗伯茨,
Paul Barham,保罗·巴勒姆,
Hyung Won Chung,郑亨元,
Charles Sutton,查尔斯·萨顿,
Sebastian Gehrmann,塞巴斯蒂安·盖尔曼,
pathways,路径,
Daniel Crankshaw,丹尼尔·克兰克斯肖,
Gur-Eyal Sela,古尔-埃亚尔·塞拉,
Xiangxi Mo,莫向西,
Corey Zumar,科里·祖马尔,
Alexey Tumanov,阿列克谢·图马诺夫,
ACM Symposium on Cloud Computing,ACM云计算研讨会,
Xin Wang,王鑫,
Guilio Zhou,周贵礼,
Michael J Franklin,迈克尔·J·富兰克林,
USENIX Symposium on Networked Systems Design and Implementation,USENIX网络系统设计与实现研讨会,
Weihao Cui,崔伟豪,
Han Zhao,赵涵,
Quan Chen,陈全,
Hao Wei,魏浩,
Zirui Li,李子睿,
Deze Zeng,曾德泽,
Chao Li,李超,
Minyi Guo,郭敏毅,
DNN,深度神经网络,
USENIX Annual Technical Conference,USENIX年度技术大会,
Tri Dao,Tri Dao,
Dan Fu,傅丹,
Stefano Ermon,斯特凡诺·埃尔蒙,
Atri Rudra,阿特里·鲁德拉,
Christopher Ré,克里斯托弗·雷,
Advances in Neural Information Processing Systems,神经信息处理系统进展,
Jiarui Fang,方佳瑞,
Yang Yu,于洋,
Chengduo Zhao,赵成多,
Jie Zhou,周杰,
TurboTransformers,TurboTransformers,
ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming,ACM SIGPLAN并行编程原理与实践研讨会,
FastAPI,FastAPI,
Pin Gao,高品,
Lingfan Yu,余凌帆,
Yongwei Wu,吴永威,
Jinyang Li,李金阳,
cellular batching,细胞批处理,
EuroSys Conference,EuroSys会议,
Amir Gholami,阿米尔·戈拉米,
Zhewei Yao,姚哲玮,
Sehoon Kim,金世勋,
Michael W Mahoney,迈克尔·W·马奥尼,
Kurt Keutzer,库尔特·科策,
RiseLab Medium Post,RiseLab Medium文章,
Github,Github,
bard.google.com,bard.google.com,
Arpan Gujarati,阿尔潘·古贾拉特,
Reza Karimi,雷扎·卡里米,
Safya Alzayat,萨菲娅·阿尔扎亚特,
Wei Hao,韦浩,
Antoine Kaufmann,安托万·考夫曼,
Ymir Vigfusson,伊米尔·维格福森,
Jonathan Mace,乔纳森·梅斯,
DNNs,深度神经网络,
USENIX Symposium on Operating Systems Design and Implementation,USENIX操作系统设计与实现研讨会,
Mingcong Han,韩明聪,
Hanze Zhang,张瀚泽,
Rong Chen,陈荣,
Haibo Chen,陈海波,
Microsecond-scale Preemption,微秒级抢占,
GPU-accelerated DNN Inferences,GPU加速的深度神经网络推理,
Kaiming He,何恺明,
Xiangyu Zhang,张祥宇,
Shaoqing Ren,任少卿,
Jian Sun,孙健,
Deep residual learning,深度残差学习,
IEEE conference on computer vision and pattern recognition,IEEE计算机视觉与模式识别会议,
Chien-Chin Huang,黄千真,
Gu Jin,金鼓,
Swapadvisor,Swapadvisor,
International Conference on Architectural Support for Programming Languages and Operating Systems,编程语言与操作系统架构支持国际会议,
Paras Jain,帕拉斯·贾殷,
Ajay Jain,阿杰·贾殷,
Aniruddha Nrusimha,阿尼鲁达·努里西玛,
Pieter Abbeel,皮特·阿贝尔,
Checkmate,Checkmate,
tensor rematerialization,张量重计算,
Block 0,块 0,
Figure 4,图 4,
compaction,压缩,
memory fragmentation,内存碎片化,
pre-allocated chunk space,预分配的块空间,
memory management systems,内存管理系统,
Method,方法,
LLM serving engine,大语言模型服务引擎,
centralized scheduler,集中式调度器,
distributed GPU workers,分布式GPU工作节点,
KV cache manager,KV缓存管理器,
paged fashion,分页方式,
physical KV cache memory,物理KV缓存内存,
Fig. 4,图 4,
§4.1,第4.1节,
§4.2,第4.2节,
§4.3,第4.3节,
§4.4,第4.4节,
§4.5,第4.5节,
§4.6,第4.6节,
variable length input and output sequences,变长输入和输出序列,
continuous keys and values,连续的键和值,
non-contiguous memory space,非连续内存空间,
block size (B),块大小(B),
key block,键块,
value block,值块,
attention heads,注意力头,
layers,层,
block tables,块表,
Figure 5,图 5,
row vector,行向量,
Eq. 4,公式 4,
block-wise computation,按块计算,
fine-grained batching,细粒度批处理,
memory management,内存管理,
13B parameter OPT model,130亿参数OPT模型,
hidden state size,隐藏状态大小,
number of layers,层数,
bytes per FP16,FP16每字节数,
concurrent GPUs,并发GPU,
inefficient memory management,低效内存管理,
computation speed,计算速度,
H100,H100,
FLOPS,FLOPS,
LLM services,大语言模型服务,
random samples,随机样本,
input prompt,输入提示,
autoregressive generation,自回归生成,
context and position,上下文和位置,
sharing pattern,共享模式,
decoding process,解码过程,
input and output lengths,输入和输出长度,
prompt lengths,提示长度,
available memory,可用内存,
incoming requests,新到达请求,
ongoing generation,正在进行的生成,
deleting,删除,
swapping out,换出,
Block 1,块 1,
Block 2,块 2,
Logical KV blocks,逻辑KV块,
Request,请求,
Figure 6,图6,
physical KV blocks,物理KV块,
KV block manager,KV块管理器,
mapping,映射,
logical and physical KV blocks,逻辑与物理KV块,
filled positions,已填充位置,
memory waste,内存浪费,
prompt computation,提示计算,
logical block,逻辑块,
prefill step,预填充阶段,
self-attention algorithm,自注意力算法,
autoregressive generation phase,自回归生成阶段,
autoregressive decoding step,自回归解码步骤,
PagedAttention algorithm,分页注意力算法,
#filled record,已填充记录,
decoding iteration,解码迭代,
candidate sequences,候选序列,
input tokens,输入令牌,
Figure 7,图7,
two requests,两个请求,
21 GB,21 GB,
264 GB,264 GB,
Max. # KV cache slots,最大KV缓存槽位数,
15.7K,15.7K,
9.7K,9.7K,
60.1K,60.1K,
block-wise matrix multiplication,分块矩阵乘法,
intermediate results,中间结果,
all-reduce operation,全归约操作,
attention operator,注意力算子,
attention head dimension,注意力头维度,
SPMD process,SPMD进程,
multi-head attention,多头注意力,
model parallel execution,模型并行执行,
model shard,模型分片,
GPU workers,GPU工作节点,
control message,控制消息,
input token IDs,输入令牌ID,
block table,块表,
broadcasts,广播,
attention layers,注意力层,
all-reduce communication primitive,全归约通信原语,
sampled tokens,采样令牌,
step inputs,步输入,
frontend,前端,
GPU-based inference engine,基于GPU的推理引擎,
OpenAI API,OpenAI API,
sampling parameters,采样参数,
maximum sequence length,最大序列长度,
beam width k,束宽k,
vLLM engine,vLLM引擎,
Python,Python,
C++/CUDA code,C++/CUDA代码,
control-related components,控制相关组件,
scheduler,调度器,
block manager,块管理器,
custom CUDA kernels,自定义CUDA内核,
model executor,模型执行器,
LLMs,大语言模型,
iteration t,第t次迭代,
model,模型,
token,令牌,
xn+t,xn+t,
"P (xn+t+1 | x1, . . . , xn+t )","P (xn+t+1 | x1, . . . , xn+t)",
key vectors,键向量,
"k1, . . . , kn+t","k1, . . . , kn+t",
value vectors,值向量,
"v1, . . . , vn+t","v1, . . . , vn+t",
cached,缓存,
users,用户,
end-of-sequence (<eos>) token,序列结束（<eos>）令牌,
data dependency,数据依赖,
matrix-vector multiplication,矩阵-向量乘法,
GPU computation,GPU计算,
single request,单个请求,
Batching Techniques for LLMs,大语言模型的批处理技术,
compute utilization,计算利用率,
serving LLMs,服务大语言模型,
batching multiple requests,批处理多个请求,
overhead,开销,
amortized,分摊,
computational overhead,计算开销,
naive batching strategy,简单批处理策略,
queueing delays,排队延迟,
straightforward batching technique,直接批处理技术,
pad,填充,
GPU computation and memory,GPU计算和内存,
fine-grained batching mechanisms,细粒度批处理机制,
iteration-level scheduling,迭代级调度,
traditional methods,传统方法,
request level,请求级别,
iteration level,迭代级别,
completed requests,已完成的请求,
special GPU kernels,特殊GPU内核,
beam width,束宽,
req/s,请求/秒,
Figure 14,图14,
OPT-13B,OPT-13B,
Alpaca dataset,Alpaca数据集,
6.2 Basic Sampling,6.2 基本采样,
basic sampling,基本采样,
one sample per request,每请求一个样本,
ShareGPT dataset,ShareGPT数据集,
Fig. 12,图12,
request rate,请求率,
Orca (Oracle),Orca（Oracle）,
Orca (Max),Orca（Max）,
fine-grained scheduling mechanism,细粒度调度机制,
Fig. 13a,图13a,
OPT-175B,OPT-175B,
GPU memory space,GPU内存空间,
short sequences,短序列,
Orca (Pow2),Orca（Pow2）,
FT (bs 8),FT (批大小 8),
vLLM (bs 32),vLLM (批大小 32),
FT (bs 32),FT (批大小 32),
Block size,块大小,
Figure 16,图 16,
translation workload,翻译工作负载,
common prefix,公共前缀,
80 tokens,80 个令牌,
5 examples,5 个示例,
341 tokens,341 个令牌,
Figure 17,图 17,
chatbot workload,聊天机器人工作负载,
Fig. 10,图 10,
LLaMA-13B,LLaMA-13B,
multilingual,多语言,
WMT16,WMT16,
English-to-German translation dataset,英译德翻译数据集,
prefixes,前缀,
instruction,指令,
one-shot,单样本,
few-shot,少样本,
chatting history,聊天历史,
user query,用户查询,
OPT-13B model,OPT-13B 模型,
context length,上下文长度,
conversation rounds,对话轮次,
Orca baselines,Orca 基线,
buddy allocation algorithm,伙伴分配算法,
PagedAttention kernel,分页注意力内核,
§7.2,第7.2节,
Fig. 7,图7,
greedy decoding,贪婪解码,
sampling,采样,
LLM applications,大语言模型应用,
program assistants,程序助手,
output sequence,输出序列,
Fig. 9,图9,
k = 4,k = 4,
reference counts,引用计数,
parallel decoding,并行解码,
system prompt,系统提示,
task input,任务输入,
actual task input,实际任务输入,
outputs,输出,
Figure 11,图11,
PyTorch,PyTorch,
Transformers,Transformers,
NCCL,NCCL,
Transformer layer,Transformer层,
kernel launch overheads,内核启动开销,
Fused reshape and block write,融合重塑与块写入,
attention kernel,注意力内核,
block read,块读取,
attention operations,注意力操作,
GPU warp,GPU warp,
variable sequence lengths,可变序列长度,
request batch,请求批次,
Fused block copy,融合块复制,
cudaMemcpyAsync API,cudaMemcpyAsync API,
fork,分叉,
append,追加,
free,释放,
output sequences,输出序列,
input sequence,输入序列,
stopping condition,停止条件,
prefix sharing,前缀共享,
workloads,工作负载,
Fig. 5,图5,
physical memory,物理内存,
query vector qi,查询向量qi,
query token,查询令牌,
forth,第四个,
key vectors Kj,键向量Kj,
value vectors Vj,值向量Vj,
attention score Ai j,注意力分数Aij,
final attention output oi,最终注意力输出oi,
memory manager,内存管理器,
logical pages,逻辑页,
physical pages,物理页,
GPU DRAM,GPU DRAM,
transformer architecture,变压器架构,
specialized serving systems,专用服务系统,
GPU kernel optimizations,GPU内核优化,
advanced batching mechanisms,高级批处理机制,
parameter sharing,参数共享,
GPU utilization,GPU利用率,
memory utilization,内存利用率,
working sets,工作集,
fine-grained scheduling,细粒度调度,
interleaving,交错执行,
accelerators,加速器,
compute capability,计算能力,
memory capacity,内存容量,
memory bottleneck,内存瓶颈,
FlexGen,FlexGen,
weights,权重,
Figure 15,图15,
Alpaca trace,Alpaca轨迹,
Page-dAttention,Page-dAttention,
memory saving,内存节省,
parallel sequences,并行序列,
beam widths,束宽,
number of blocks,块数,
total blocks,总块数,
shared prefix,共享前缀,
Latency of attention kernels,注意力内核的延迟,
End-to-end latency with different block sizes,不同块大小的端到端延迟,
Figure 18,图18,
Ablation experiments,消融实验,
memory fragmentation and reservation,内存碎片化和预留,
Ablation Studies,消融研究,
Kernel Microbenchmark,内核微基准测试,
dynamic block mapping,动态块映射,
GPU operations,GPU操作,
block read/writes,块读/写,
Fig. 18a,图18a,
attention kernel latency,注意力内核延迟,
Linear,线性,
§6,第6节,
BlockSize,块大小,
performance of vLLM,vLLM的性能,
GPU’s parallelism,GPU的并行性,
internal fragmentation,内部碎片,
probability of sharing,共享概率,
Fig. 18b,图18b,
traces,轨迹,
fixed request rates,固定请求速率,
block sizes from 16 to 128,块大小从16到128,
block size 16 and 32,块大小16和32,
larger block sizes,更大的块大小,
sequences become shorter than the block sizes,序列比块大小更短,
efficiently utilize the GPU,高效利用GPU,
avoid significant internal fragmentation,避免显著的内部碎片化,
most workloads,大多数工作负载,
default block size,默认块大小,
preemptive request scheduling,抢占式请求调度,
model accuracy,模型精度,
longer sequences,更长序列,
larger models,更大模型,
complex decoding algorithms,复杂解码算法,
serving performance,服务性能,
distributed LLM serving engine,分布式大语言模型服务引擎,
generation,生成,
serving procedures,服务流程,
typical LLMs,典型大语言模型,
MemoryManagement,内存管理,
ExistingSystems,现有系统,
tensors,张量,
contiguous memory,连续内存,
LLM serving systems,大语言模型服务系统,
Fig. 3,图3,
request A,请求A,
request B,请求B,
chunk pre-allocation scheme,块预分配方案,
memory wastes,内存浪费,
reserved slots,预留槽位,
external fragmentation,外部碎片,
buddy allocator,伙伴分配器,
generated tokens,生成的标记,
effective memory,有效内存,
Task output,任务输出,
Sequence A,序列A,
Sequence B,序列B,
LLM output,大语言模型输出,
Figure 10,图10,
shared library,共享库,
prompt phase computation,提示阶段计算,
sea otter,海獭,
peppermint,薄荷,
plush girafe,毛绒长颈鹿,
cheese,奶酪,
I love you,我爱你,
Distributed Execution,分布式执行,
parameter sizes,参数规模,
single GPU,单个GPU,
model parallel fashion,模型并行方式,
distributed settings,分布式环境,
Megatron-LM,Megatron-LM,
tensor model parallelism strategy,张量模型并行策略,
SPMD,单程序多数据,
Single Program Multiple Data,单程序多数据,
linear layers,线性层,
Request rate (req/s),请求速率（每秒请求数）,
1 GPU,1个GPU,
OPT-66B,OPT-66B,
4 GPUs,4个GPU,
8 GPUs,8个GPU,
Recompute,重新计算,
Swap in,换入,
Swap out,换出,
Swap in + out,换入+换出,
Swap,交换,
(a) Microbenchmark,(a) 微基准测试,
(b) End-to-end performance,(b) 端到端性能,
Figure 19,图19,
1 slot future used,1个槽位未来使用,
(reserved),（保留）,
507 slots never used,507个槽位从未使用,
(Internal fragmentation),（内部碎片）,
current iteration,当前迭代,
1 slot for,1个槽位用于,
generated token,生成的令牌,
Figure 3. KV cache memory management in existing systems.,图3. 现有系统中的KV缓存内存管理。,
The token in each memory slot represents its KV cache.,每个内存槽位中的令牌代表其KV缓存。,
Note the same tokens can have different KV cache when at different positions.,注意，相同令牌在不同位置时可能具有不同的KV缓存。,
3 Memory Challenges inLLMServing,3 大型语言模型服务中的内存挑战,
Susan Zhang,苏珊·张,
Stephen Roller,斯蒂芬·罗勒,
Naman Goyal,纳曼·戈亚尔,
Mikel Artetxe,米克尔·阿尔特塞特,
Moya Chen,莫亚·陈,
Shuohui Chen,陈硕辉,
Christopher Dewan,克里斯托弗·德万,
Mona Diab,莫娜·迪亚卜,
Xian Li,李先,
Xi Victoria Lin,林维克多娅·曦,
Open pre-trained transformer language models,开放预训练变换器语言模型,
arXiv preprint arXiv:2205.01068,arXiv预印本arXiv:2205.01068,
Yida Wang,王一达,
Yuanzhong Xu,徐元中,
Danyang Zhuo,卓丹阳,
Eric P Xing,邢立明,
Alpa,Alpa,
Automating Inter-and Intra-Operator Parallelism for Distributed Deep Learning,分布式深度学习中算子间与算子内并行性的自动化,
OSDI 22,OSDI 22,
Zhe Zhou,周哲,
Xuechao Wei,魏学超,
Jiejing Zhang,张洁静,
Guangyu Sun,孙广宇,
PetS,PetS,
A Unified Framework for Parameter-Efficient Transformers Serving,一种用于参数高效Transformer服务的统一框架,
USENIX ATC 22,USENIX ATC 22,
parallel generation (parallel size = 2),并行生成（并行大小 = 2）,
parallel generation (parallel size = 4),并行生成（并行大小 = 4）,
parallel generation (parallel size = 6),并行生成（并行大小 = 6）,
beam search (beam width = 2),束搜索（束宽 = 2）,
Table 1. Model sizes and server configurations.,表1. 模型规模和服务器配置。,
Model size,模型规模,
1-shot prefix prompt,单样本前缀提示,
5-shot prefix prompt,五样本前缀提示,
vLLM (bs 8),vLLM（批大小 8）,
(Max),(最大),
(Pow2),(幂2),
(Oracle),(甲骨文),
Fi,图,
gure 12,12,
Single sequence generation with OPT models on the ShareGPT and Alpaca dataset,在ShareGPT和Alpaca数据集上使用OPT模型进行单序列生成,
(a) ShareGPT,(a) ShareGPT,
(b) Alpaca,(b) Alpaca,
Figure 13,图13,
2038 slots never used,2038 个插槽从未使用,
2 slots future used,2 个插槽将来使用,
reserved,保留,
7 KV cache states for,7 个KV缓存状态用于,
request A’s prompt,请求A的提示,
3 KV cache states for,3 个KV缓存状态用于,
Block 3,块 3,
Block 6,块 6,
Block 7,块 7,
Block 5,块 5,
Block 4,块 4,
Block 8,块 8,
Block 9,块 9,
Block 12,块 12,
Beam candidate 0,束候选 0,
Beam candidate 1,束候选 1,
Four score and seven years ago,四-score 七-year 前,
our fathers,我們的父親,
brought,帶來,
Physical block,物理块,
A100,A100,
4×A100,4×A100,
8×A100-80GB,8×A100-80GB,
Total GPU memory,总GPU内存,
40 GB,40 GB,
160 GB,160 GB,
640 GB,640 GB,
26 GB,26 GB,
132 GB,132 GB,
346 GB,346 GB,
Memory for KV cache,KV缓存内存,
12 GB,12 GB,
years,年,
mothers,母亲,
Sample,样本,
Ref count: 2 → 1,引用计数：2 → 1,
Block 10,块 10,
Block 11,块 11,
Four,四,
score,二十,
seven,七,
ago,前,
our,我们的,
fathers,父亲,
brought forth,创立,
of,的,
times,时代,
Block,块,
Shard 1,分片 1,
Engine,引擎,
Worker N - 1,工作节点 N - 1,
Shard N - 1,分片 N - 1,
Query,查询,
vector,向量,
CPU Block,CPU块,
Allocator,分配器,
GPU Block,GPU块,
Worker 0,工作节点0,
Shard 0,分片0,
Worker 1,工作节点1,
number,数字,
Key and value vectors,键向量和值向量,
