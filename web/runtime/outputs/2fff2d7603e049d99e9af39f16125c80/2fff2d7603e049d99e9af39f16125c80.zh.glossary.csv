source,target,tgt_lng
Jakob,雅各布,
Ashish,阿希什,
Illia,伊利亚,
Noam,诺姆,
Niki,妮基,
Llion,利昂,
Lukasz,卢卡什,
Aidan,艾丹,
Google Brain,谷歌大脑,
Google Research,谷歌研究,
31st Conference on Neural Information Processing Systems (NIPS 2017),第31届神经信息处理系统大会（NIPS 2017）,
"Long Beach, CA, USA",美国加利福尼亚州长滩,
Transformer,Transformer,
RNNs,循环神经网络,
self-attention,自注意力,
scaled dot-product attention,缩放点积注意力,
multi-head attention,多头注意力,
parameter-free position representation,无参数位置表示,
tensor2tensor,tensor2tensor,
transduction model,转导模型,
input,输入,
output,输出,
convolution,卷积,
Model Architecture,模型架构,
neural sequence transduction models,神经序列转导模型,
encoder-decoder structure,编码器-解码器结构,
encoder,编码器,
decoder,解码器,
input sequence,输入序列,
symbol representations,符号表示,
continuous representations,连续表示,
output sequence,输出序列,
symbols,符号,
auto-regressive,自回归,
Google,谷歌,
Attention Is All You Need,注意力就是全部,
Ashish Vaswani,阿希什·瓦斯瓦尼,
Noam Shazeer,诺姆·沙泽尔,
Niki Parmar,妮基·帕尔马,
Jakob Uszkoreit,雅各布·乌什科莱特,
Llion Jones,利昂·琼斯,
Aidan N. Gomez,艾丹·N·戈麦斯,
University of Toronto,多伦多大学,
Łukasz Kaiser,卢卡什·凯泽,
Illia Polosukhin,伊利亚·波洛苏金,
Abstract,摘要,
sequence transduction models,序列转导模型,
recurrent or convolutional neural networks,循环或卷积神经网络,
attention mechanism,注意力机制,
machine translation tasks,机器翻译任务,
WMT 2014 English-to-German translation task,WMT 2014年英译德任务,
BLEU,BLEU值,
WMT 2014 English-to-French translation task,WMT 2014年英译法任务,
state-of-the-art BLEU score,最先进的BLEU分数,
eight GPUs,八块GPU,
training costs,训练成本,
English constituency parsing,英文成分句法分析,
Recurrent neural networks,循环神经网络,
long short-term memory,长短期记忆,
gated recurrent,门控循环,
sequence modeling,序列建模,
transduction problems,转换问题,
language modeling,语言建模,
machine translation,机器翻译,
encoder-decoder architectures,编码器-解码器架构,
hidden states ht,隐状态ht,
previous hidden state ht−1,前一隐状态ht−1,
input for position t,位置t的输入,
sequential computation,顺序计算,
Attention mechanisms,注意力机制,
global dependencies,全局依赖,
Extended Neural GPU,扩展神经GPU,
ByteNet,ByteNet,
ConvS2S,ConvS2S,
convolutional neural networks,卷积神经网络,
hidden representations,隐层表示,
intra-attention,内部注意力,
reading comprehension,阅读理解,
abstractive summarization,抽象摘要,
textual entailment,文本蕴含,
sentence representations,句子表示,
End-to-end memory networks,端到端记忆网络,
question answering,问答,
length n,长度 n,
representation dimensionality d,表示维度 d,
state-of-the-art models,最先进的模型,
machine translations,机器翻译,
word-piece,词片段,
byte-pair,字节对,
computational performance,计算性能,
very long sequences,非常长的序列,
neighborhood of size r,大小为 r 的邻域,
output position,输出位置,
maximum path length,最大路径长度,
future work,未来工作,
convolutional layer,卷积层,
kernel width k < n,卷积核宽度 k < n,
contiguous kernels,连续卷积核,
dilated convolutions,空洞卷积,
longest paths,最长路径,
recurrent layers,循环层,
separable convolutions,可分离卷积,
complexity,复杂度,
point-wise feed-forward layer,逐点前馈层,
attention distributions,注意力分布,
appendix,附录,
attention heads,注意力头,
syntactic and semantic structure,句法和语义结构,
Training,训练,
training regime,训练机制,
WMT 2014 English-German dataset,WMT 2014 英德数据集,
4.5 million sentence pairs,450万句对,
byte-pair encoding,字节对编码,
shared source-target vocabulary,共享源目标词汇表,
37000 tokens,37000个标记,
English-French,英法,
WMT 2014 English-French dataset,WMT 2014 英法数据集,
36M sentences,3600万句子,
word-piece vocabulary,词片段词汇表,
32000 word-piece vocabulary,32000词片段词汇表,
sentence pairs,句子对,
batched together,批量组合,
approximate sequence length,近似序列长度,
training batch,训练批次,
25000 source tokens,25000个源标记,
25000 target tokens,25000个目标标记,
Hardware and Schedule,硬件与时间安排,
NVIDIA P100 GPUs,NVIDIA P100 GPU,
base models,基础模型,
hyperparameters,超参数,
training step,训练步,
"100,000 steps",10万步,
12 hours,12小时,
big models,大模型,
table 3,表3,
step time,步时,
"300,000 steps",30万步,
3.5 days,3.5天,
Optimizer,优化器,
Adam optimizer,Adam优化器,
β1 = 0.9,β1 = 0.9,
β2 = 0.98,β2 = 0.98,
ε = 10−9,ε = 10−9,
learning rate,学习率,
Figure 2,图2,
model,模型,
information,信息,
representation subspaces,表示子空间,
positions,位置,
attention head,注意力头,
parameter matrices,参数矩阵,
W Q i,W Q i,
Rdmodel×dk,Rdmodel×dk,
W Ki,W Ki,
W Vi,W Vi,
Rhdv×dmodel,Rhdv×dmodel,
W O,W O,
h = 8,h = 8,
attention layers,注意力层,
heads,头,
dmodel/h,dmodel/h,
dimension,维度,
computational cost,计算成本,
single-head attention,单头注意力,
full dimensionality,完整维度,
encoder-decoder attention,编码器-解码器注意力,
queries,查询,
decoder layer,解码器层,
memory keys,记忆键,
values,值,
sequence-to-sequence models,序列到序列模型,
self-attention layers,自注意力层,
previous layer,前一层,
leftward information flow,向左的信息流,
auto-regressive property,自回归属性,
masking out,掩码屏蔽,
softmax,softmax,
Position-wise Feed-Forward Networks,位置级前馈网络,
feed-forward network,前馈网络,
linear transformations,线性变换,
ReLU activation,ReLU激活,
convolutions,卷积,
kernel size 1,核大小1,
dmodel = 512,dmodel = 512,
dff = 2048,dff = 2048,
Embeddings,嵌入,
query,查询,
key,键,
value,值,
matrix Q,矩阵Q,
matrix K,矩阵K,
matrix V,矩阵V,
additive attention,加性注意力,
dot-product attention,点积注意力,
multiplicative attention,乘性注意力,
hidden layer,隐藏层,
scaling factor,缩放因子,
theoretical complexity,理论复杂度,
space-efficient,空间高效,
matrix multiplication,矩阵乘法,
linear projections,线性投影,
dmodel,dmodel,
h times,h次,
GNMT + RL Ensemble,GNMT + RL 集成,
ConvS2S Ensemble,ConvS2S 集成,
Transformer (base model),Transformer（基础模型）,
3.3 · 1018,3.3 · 10^18,
Transformer (big),Transformer（大模型）,
Residual Dropout,残差丢弃,
dropout,丢弃率,
Pdrop = 0.1,Pdrop = 0.1,
Label Smoothing,标签平滑,
εls = 0.1,εls = 0.1,
WMT 2014,WMT 2014,
English-to-German,英译德,
English-to-French,英译法,
Table 2,表2,
P100 GPUs,P100 GPU,
base model,基础模型,
big model,大模型,
checkpoints,检查点,
beam search,束搜索,
beam size,束大小,
length penalty α = 0.6,长度惩罚 α = 0.6,
development set,开发集,
maximum output length,最大输出长度,
input length + 50,输入长度 + 50,
positional embedding,位置嵌入,
sinusoids,正弦函数,
big,大,
300K,30万,
newstest2013,newstest2013,
checkpoint averaging,检查点平均,
attention key,注意力键,
value dimensions,值维度,
computation,计算,
Section 3.2.2,第3.2.2节,
model quality,模型质量,
compatibility function,兼容性函数,
dot product,点积,
learned positional embeddings,学习到的位置嵌入,
Penn Treebank,宾州树库,
WSJ,华尔街日报,
Wall Street Journal,华尔街日报,
4-layer transformer,4层Transformer,
dmodel = 1024,dmodel = 1024,
40K training sentences,4万条训练句子,
semi-supervised setting,半监督设置,
high-confidence,高置信度,
BerkleyParser corpora,BerkleyParser语料库,
17M sentences,1700万句子,
vocabulary,词汇表,
16K tokens,1.6万个词符,
32K tokens,3.2万个词符,
Figure 1,图1,
The Transformer,Transformer模型,
point-wise,逐点,
fully connected layers,全连接层,
Encoder and Decoder Stacks,编码器和解码器堆栈,
N = 6 identical layers,N = 6个相同层,
multi-head self-attention mechanism,多头自注意力机制,
position-wise fully connected feed-forward network,位置级全连接前馈网络,
residual connection,残差连接,
layer normalization,层归一化,
LayerNorm(x + Sublayer(x)),LayerNorm(x + 子层(x)),
embedding layers,嵌入层,
dimension dmodel = 512,维度dmodel = 512,
decoder inserts a third sub-layer,解码器插入第三个子层,
output of the encoder stack,编码器堆栈的输出,
masking,掩码,
output embeddings are offset by one position,输出嵌入偏移一个位置,
Attention,注意力,
attention function,注意力函数,
key-value pairs,键值对,
vectors,向量,
weighted sum,加权和,
Table 3 row (E),表3第(E)行,
sinusoidal version,正弦版本,
recurrent,循环,
convolutional layers,卷积层,
variable-length sequence,变长序列,
sequence transduction encoder,序列转导编码器,
sequence transduction decoder,序列转导解码器,
computational complexity,计算复杂度,
parallelized,并行化,
sequential operations,顺序操作,
path length,路径长度,
long-range dependencies,长距离依赖,
input and output sequences,输入和输出序列,
Table 1,表1,
Dyer et al. (2016),戴尔等人（2016）,
generative,生成式,
incr,增加,
input length,输入长度,
Table 4,表4,
task-specific tuning,任务特定调优,
previously reported models,先前报道的模型,
Recurrent Neural Network Grammar,递归神经网络语法,
RNN sequence-to-sequence models,RNN序列到序列模型,
Berkeley-Parser,伯克利解析器,
training set,训练集,
sentences,句子,
Conclusion,结论,
sequence transduction model,序列转换模型,
multi-headed self-attention,多头自注意力,
translation tasks,翻译任务,
trained,训练,
faster,更快,
architectures,架构,
WMT 2014 English-to-German,WMT 2014 英译德,
WMT 2014 English-to-French,WMT 2014 英译法,
state of the art,最先进的水平,
best model,最佳模型,
ensembles,集成模型,
attention-based models,基于注意力的模型,
input and output modalities,输入和输出模态,
text,文本,
"local, restricted attention mechanisms",局部受限注意力机制,
large inputs and outputs,大规模输入和输出,
images,图像,
audio,音频,
video,视频,
generation,生成,
sequential,顺序性,
research goals,研究目标,
code,代码,
tensorflow/tensor2tensor,tensorflow/tensor2tensor,
O(n),O(n),
Convolutional,卷积,
O(k · n · d2),O(k · n · d²),
O(1),O(1),
O(logk(n)),O(logₖ(n)),
Self-Attention (restricted),自注意力（受限）,
O(r · n · d),O(r · n · d),
O(n/r),O(n/r),
3.5 Positional Encoding,3.5 位置编码,
recurrence,循环,
sequence,序列,
position,位置,
tokens,标记,
stacks,堆栈,
positional encodings,位置编码,
sine,正弦,
cosine functions,余弦函数,
frequencies,频率,
pos,位置,
sinusoid,正弦波,
wavelengths,波长,
geometric progression,几何级数,
10000 · 2π,10000 · 2π,
offset,偏移,
P Epos+k,PEₚₒₛ₊ₖ,
P Epos,PEₚₒₛ,
linear function,线性函数,
relative positions,相对位置,
Nal Kalchbrenner,纳尔·卡尔奇布伦纳,
Stephan Gouws,斯特凡·古斯,
Jimmy Lei Ba,杰米·雷·巴,
Jamie Ryan Kiros,杰米·瑞安·基罗斯,
Geoffrey E Hinton,杰弗里·辛顿,
Dzmitry Bahdanau,德米特里·巴哈纳乌,
Kyunghyun Cho,千京贤,
Yoshua Bengio,约书亚·本吉奥,
Denny Britz,丹尼·布里茨,
Anna Goldie,安娜·戈尔迪,
Minh-Thang Luong,明恒·梁,
Quoc V. Le,觉·V·黎,
Jianpeng Cheng,程建鹏,
Li Dong,李东,
Mirella Lapata,米雷拉·拉帕塔,
Section 23 of WSJ,华尔街日报第23节,
Parser,解析器,
WSJ 23 F1,WSJ 23 F1分数,
Vinyals & Kaiser et al. (2014) [37],Vinyals与Kaiser等人（2014）[37],
Petrov et al. (2006) [29],Petrov等人（2006）[29],
Zhu et al. (2013) [40],朱等人（2013）[40],
Dyer et al. (2016) [8],戴尔等人（2016）[8],
"WSJ only, discriminative",仅使用WSJ，判别式,
Transformer (4 layers),Transformer（4层）,
Maximum path lengths,最大路径长度,
per-layer complexity,每层复杂度,
minimum number of sequential operations,最小顺序操作数,
Layer Type,层类型,
Complexity per Layer,每层复杂度,
Operations,操作,
O(n2 · d),O(n² · d),
O(n · d2),O(n · d²),
newstest2014,newstest2014,
ByteNet [18],ByteNet [18],
Deep-Att + PosUnk [39],Deep-Att + PosUnk [39],
GNMT + RL [38],GNMT + RL [38],
ConvS2S [9],ConvS2S [9],
MoE [32],MoE [32],
Deep-Att + PosUnk Ensemble [39],Deep-Att + PosUnk集成模型 [39],
Training Cost (FLOPs),训练成本（FLOPs）,
EN-DE,英-德,
EN-FR,英-法,
learned embeddings,学习得到的嵌入,
input tokens,输入标记,
output tokens,输出标记,
dimension dmodel,维度dmodel,
learned linear transformation,学习得到的线性变换,
softmax function,softmax函数,
decoder output,解码器输出,
predicted next-token probabilities,预测的下一个标记概率,
weight matrix,权重矩阵,
pre-softmax linear transformation,softmax前的线性变换,
√dmodel,√dmodel,
semi-supervised,半监督,
Huang & Harper (2009) [14],黄和哈珀（2009）[14],
McClosky et al. (2006) [26],麦克洛斯基等人（2006）[26],
Luong et al. (2015) [23],梁等人（2015）[23],
multi-task,多任务,
perplexities,困惑度,
ff,前馈网络,
train,训练,
PPL,困惑度,
params,参数,
steps,步数,
dev,开发集,
Figure 3,图3,
long-distance dependencies,长距离依赖,
encoder self-attention,编码器自注意力,
layer 5 of 6,6层中的第5层,
verb ‘making’,动词‘making’,
completing the phrase ‘making...more difficult’,完成短语‘making...more difficult’,
word ‘making’,单词‘making’,
Different colors,不同颜色,
English-to-German translation,英译德,
K80,K80,
K40,K40,
M40,M40,
P100,P100,
TFLOPS,每秒万亿次浮点运算,
Mitchell P Marcus,米切尔·P·马库斯,
Mary Ann Marcinkiewicz,玛丽·安·马辛凯维奇,
Beatrice Santorini,碧翠丝·桑托里尼,
The penn treebank,宾夕法尼亚树库,
Computational linguistics,计算语言学,
David McClosky,大卫·麦克洛斯基,
Eugene Charniak,尤金·查尔尼阿克,
Mark Johnson,马克·约翰逊,
NAACL,北美计算语言学协会,
ACL,计算语言学协会,
Ankur Parikh,安库尔·帕里克,
Oscar Täckström,奥斯卡·塔克斯特伦,
Dipanjan Das,迪潘詹·达斯,
A decomposable attention model,可分解的注意力模型,
Empirical Methods in Natural Language Processing,自然语言处理经验方法,
Romain Paulus,罗曼·保罗斯,
Caiming Xiong,熊彩明,
Richard Socher,理查德·索彻,
A deep reinforced model for abstractive summarization,用于抽象摘要的深度强化模型,
arXiv preprint,arXiv预印本,
Slav Petrov,斯拉夫·彼得罗夫,
Leon Barrett,莱昂·巴雷特,
Romain Thibaux,罗曼·蒂博,
Dan Klein,丹·克莱因,
"Learning accurate, compact, and interpretable tree annotation",学习准确、紧凑且可解释的树标注,
International Conference on Computational Linguistics,国际计算语言学会议,
Annual Meeting of the ACL,计算语言学协会年会,
Ofir Press,奥菲尔·普雷斯,
Lior Wolf,利奥尔·沃尔夫,
Using the output embedding to improve language models,使用输出嵌入改进语言模型,
Rico Sennrich,里科·森内里希,
Barry Haddow,巴里·哈多,
Alexandra Birch,亚历山德拉·伯奇,
Neural machine translation of rare words with subword units,使用子词单元进行罕见词的神经机器翻译,
Azalia Mirhoseini,阿扎莉亚·米尔侯赛尼,
Krzysztof Maziarz,克里斯托夫·马齐亚尔兹,
Andy Davis,安迪·戴维斯,
Quoc Le,郭乐,
Geoffrey Hinton,杰弗里·欣顿,
Jeff Dean,杰夫·迪恩,
Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,超大规模神经网络：稀疏门控的专家混合层,
Nitish Srivastava,尼提什·斯里瓦斯塔瓦,
Alex Krizhevsky,亚历克斯·克里热夫斯基,
Ilya Sutskever,伊利亚·苏茨克维尔,
Ruslan Salakhutdinov,鲁斯兰·萨拉胡丁诺夫,
Dropout: a simple way to prevent neural networks from overfitting,Dropout：防止神经网络过拟合的简单方法,
Journal of Machine Learning Research,机器学习研究杂志,
Sainbayar Sukhbaatar,赛恩巴亚尔·苏赫巴塔尔,
Arthur Szlam,亚瑟·兹拉姆,
Jason Weston,杰森·韦斯顿,
Rob Fergus,罗布·弗格斯,
C. Cortes,C·科尔特斯,
N. D. Lawrence,N·D·劳伦斯,
D. D. Lee,D·D·李,
M. Sugiyama,M·杉山,
R. Garnett,R·加内特,
Advances in Neural Information Processing Systems,神经信息处理系统进展,
"Curran Associates, Inc.",柯伦联合公司,
Oriol Vinyals,奥里奥尔·维尼亚尔斯,
Quoc VV Le,郭VV·乐,
Sequence to sequence learning with neural networks,使用神经网络的序列到序列学习,
Christian Szegedy,克里斯蒂安·塞格迪,
Vincent Vanhoucke,文森特·范豪克,
Sergey Ioffe,谢尔盖·约菲,
Jonathon Shlens,乔纳森·施伦斯,
Zbigniew Wojna,兹比格涅夫·沃伊纳,
Rethinking the inception architecture for computer vision,重新思考计算机视觉的Inception架构,
CoRR,计算机科学成果档案,
Vinyals & Kaiser,维尼亚尔斯和凯撒,
Koo,顾,
Grammar as a foreign language,将语法作为外语,
Yonghui Wu,吴永辉,
Mike Schuster,迈克·舒斯特,
Zhifeng Chen,陈志峰,
Quoc V Le,郭V·乐,
Mohammad Norouzi,穆罕默德·诺鲁齐,
Wolfgang Macherey,沃尔夫冈·马切雷,
Maxim Krikun,马克西姆·克里昆,
Yuan Cao,曹源,
Qin Gao,高勤,
Klaus Macherey,克劳斯·马切雷,
Google’s neural machine translation system: Bridging the gap between human and machine translation,谷歌的神经机器翻译系统：弥合人与机器翻译之间的差距,
Jie Zhou,周洁,
Ying Cao,曹颖,
Xuguang Wang,王旭光,
Peng Li,李鹏,
Wei Xu,许巍,
Deep recurrent models with fast-forward connections for neural machine translation,用于神经机器翻译的具有前馈连接的深度循环模型,
Muhua Zhu,朱慕华,
Yue Zhang,张岳,
Wenliang Chen,陈伟亮,
Min Zhang,张敏,
Jingbo Zhu,朱敬波,
Fast and accurate shift-reduce constituent parsing,快速准确的移进-归约成分句法分析,
Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers),第51届计算语言学协会年会论文集（第一卷：长文）,
warmup_steps,预热步数,
training steps,训练步骤,
inverse square root,逆平方根,
step number,步骤编号,
Regularization,正则化,
Figure 4,图4,
anaphora resolution,指代消解,
Full attentions for head 5,第5注意力头的完整注意力,
Isolated attentions from just the word ‘its’,仅来自‘its’一词的孤立注意力,
attention heads 5 and 6,注意力头5和6,
residual,残差,
learning rates,学习率,
Section 22 development set,第22节开发集,
base translation model,基础翻译模型,
Figure 5,图5,
sentence,句子,
